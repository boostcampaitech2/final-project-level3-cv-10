{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conda 환경 설정\n",
    "!conda create -n convert\n",
    "!conda activate convert\n",
    "!conda install jupyter notebook\n",
    "!python3 -m ipykernel install --user --name convert --display-name \"convert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update\n",
    "!sudo apt update --yes\n",
    "!sudo apt upgrade --yes\n",
    "!sudo apt autoremove --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lib 설치\n",
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "!conda install -c conda-forge onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ai/Desktop/work/ai_boostcamp/final/pth_convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch를 ONNX로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "#https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/deeplabv3.py\n",
    "\n",
    "#용범님이 upload한 deeplayv3.py 파일이 ipynb 실행 폴더에 위치\n",
    "from deeplabv3 import deeplabv3_mobilenet_v3_large\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch_model = deeplabv3_mobilenet_v3_large().to(device)\n",
    "#용범님이 upload한 model_weights.pth 파일이 ipynb 실행 폴더에 위치\n",
    "model_path = './model_weights.pth'\n",
    "torch_model.load_state_dict(torch.load(model_path))\n",
    "torch_model.eval()\n",
    "\n",
    "batch_size = 1\n",
    "#[batch_size, channels, height, width]\n",
    "x = torch.randn(batch_size, 3, 480, 640).to(device)\n",
    "\n",
    "# 모델 변환\n",
    "torch.onnx.export(torch_model,               # 실행될 모델\n",
    "                  x,                         # 모델 입력값 (튜플 또는 여러 입력값들도 가능)\n",
    "                  \"onnx.onnx\",   # 모델 저장 경로 (파일 또는 파일과 유사한 객체 모두 가능)\n",
    "                  #export_params=True,        # 모델 파일 안에 학습된 모델 가중치를 저장할지의 여부 default True\n",
    "                  #opset_version=12,          # 모델을 변환할 때 사용할 ONNX 버전\n",
    "                  #do_constant_folding=False,  # 최적하시 상수폴딩을 사용할지의 여부 Default False\n",
    "                  verbose = True,\n",
    "                  input_names = ['actual_input_1'],   # 모델의 입력값을 가리키는 이름\n",
    "                  output_names = ['output1'] # 모델의 출력값을 가리키는 이름\n",
    "                  )\n",
    "onnx_model = onnx.load(\"onnx.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "onnx.helper.printable_graph(onnx_model.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX를 TensorFlow로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx_tf\n",
    "!sudo apt update --yes\n",
    "!sudo apt install python3-dev python3-pip python3-venv --yes\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "\n",
    "TF_PATH = \"./pb.pb\" # where the representation of tensorflow model will be stored\n",
    "ONNX_PATH = \"./onnx.onnx\" # path to my existing ONNX model\n",
    "onnx_model = onnx.load(ONNX_PATH)  # load onnx model\n",
    "\n",
    "# prepare function converts an ONNX model to an internel representation\n",
    "# of the computational graph called TensorflowRep and returns\n",
    "# the converted representation.\n",
    "tf_rep = prepare(onnx_model)  # creating TensorflowRep object\n",
    "\n",
    "# export_graph function obtains the graph proto corresponding to the ONNX\n",
    "# model associated with the backend representation and serializes\n",
    "# to a protobuf file.\n",
    "tf_rep.export_graph(TF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow를 TensorFlow Lite로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./pb.pb\") # path to the SavedModel directory\n",
    "\n",
    "#optimization\n",
    "#https://github.com/sithu31296/PyTorch-ONNX-TFLite\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.target_spec.supported_ops = [\n",
    "#     tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TFLite ops\n",
    "#     tf.lite.OpsSet.SELECT_TF_OPS  # enable TF ops\n",
    "# ]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('tflite.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "filename = './test_images/MP_SEL_SUR_000002.jpg'\n",
    "img = Image.open(filename)\n",
    "img = img.resize((640, 480))\n",
    "to_tesnsor = transforms.ToTensor()\n",
    "input_data = to_tesnsor(img)\n",
    "input_data = input_data.unsqueeze(0)\n",
    "print(input_data.size())\n",
    "to_pil = transforms.ToPILImage()\n",
    "img = to_pil(input_data.squeeze())\n",
    "img.save('./resize.jpg')\n",
    "\n",
    "#transfom compose 방법\n",
    "# input_image = Image.open(filename)\n",
    "# input_image = input_image.convert(\"RGB\")\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Resize((480, 640)),\n",
    "#     #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# input_data = preprocess(input_image)\n",
    "# input_data = input_data.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "# print(input_data.size())\n",
    "\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path='tflite.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_data = output_data.argmax(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, dpi=200)\n",
    "axes[0].imshow(output_data[0])\n",
    "axes[1].imshow(input_data[0].cpu().numpy().transpose([1, 2, 0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55558c5b5f3dd3c37f143c615d8e3fce0a754830491af7fc6afd8a1ca32d19c2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('convert': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
